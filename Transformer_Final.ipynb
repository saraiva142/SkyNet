{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8StM3k7ri1py"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carregar os dados do CSV\n",
        "data = pd.read_csv('merged_df.csv')\n",
        "\n",
        "# Exibir as primeiras linhas para verificar a estrutura\n",
        "print(data.head())\n",
        "\n",
        "# Lidando com NaNs (preenchendo com média)\n",
        "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numeric_cols] = data[numeric_cols].interpolate(method='linear', limit_direction='forward', axis=0)\n",
        "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
        "\n",
        "# Função para mapear os dados em blocos de 60\n",
        "def map_data_in_blocks(data, block_size=60):\n",
        "    sequences = []\n",
        "    for i in range(0, len(data), block_size):\n",
        "        block = data.iloc[i:i+block_size]\n",
        "        if len(block) == block_size:\n",
        "            # Usar apenas colunas numéricas\n",
        "            numeric_block = block.select_dtypes(include=[np.number])\n",
        "            sequences.append(numeric_block.mean())  # Aqui você pode calcular a média ou aplicar outra função\n",
        "    return pd.DataFrame(sequences)\n",
        "\n",
        "# Aplicando a função para mapear em blocos\n",
        "data_mapped = map_data_in_blocks(data)\n",
        "\n",
        "# Exibindo a estrutura após o mapeamento\n",
        "print(data_mapped.head())\n",
        "\n",
        "# Selecionando features e target\n",
        "features = data_mapped[[\"temperature\", \"speed\"]]\n",
        "target = data_mapped['dst']\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Função para criar sequências de dados\n",
        "def create_sequences(X, y, time_steps=60):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:i+time_steps])\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# Definindo a quantidade de timesteps (dados anteriores a serem usados para previsão)\n",
        "time_steps = 60\n",
        "\n",
        "# Criando sequências\n",
        "X_seq, y_seq = create_sequences(X_scaled, target.values, time_steps)\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "train_size = int(0.8 * len(X_seq))\n",
        "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
        "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
        "\n",
        "# Definição do modelo Transformer\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.input_projection(src)\n",
        "        tgt = src\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = self.fc_out(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Função para treinar o Transformer com regularização\n",
        "def train_transformer(X_train, y_train, X_test, y_test):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_dim = X_train.shape[2]\n",
        "    d_model = 16  # Similar à LSTM\n",
        "    nhead = 4\n",
        "    num_encoder_layers = 2\n",
        "    dim_feedforward = 64\n",
        "    dropout = 0.3  # Ajustado como no LSTM\n",
        "    model = TransformerModel(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Ajuste da taxa de aprendizado e L2\n",
        "    train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
        "    test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Ajuste do tamanho de batch\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    early_stopping_patience = 10\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(10):  # Aumente o número de épocas conforme necessário\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            #print(X_batch.shape)\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output.squeeze(), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        y_val_pred = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                val_output = model(X_batch)\n",
        "                val_loss += criterion(val_output.squeeze(), y_batch).item()\n",
        "                y_val_pred.append(val_output.cpu().numpy())\n",
        "\n",
        "        y_val_pred = np.concatenate(y_val_pred, axis=0)\n",
        "        y_val_pred = y_val_pred[:len(y_test)]\n",
        "        rmse_val = np.sqrt(mean_squared_error(y_test, y_val_pred))\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Test Loss: {val_loss/len(test_loader):.4f}, Test RMSE: {rmse_val:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == early_stopping_patience:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Treinando o Transformer\n",
        "model = train_transformer(X_train, y_train, X_test, y_test)\n"
      ]
    }
  ]
}